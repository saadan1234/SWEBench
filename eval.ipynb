{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.pyenv_mirror/user/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "swebench = load_dataset('princeton-nlp/SWE-bench', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['repo', 'instance_id', 'base_commit', 'patch', 'test_patch', 'problem_statement', 'hints_text', 'created_at', 'version', 'FAIL_TO_PASS', 'PASS_TO_PASS', 'environment_setup_commit']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'swebench.harness.run_evaluation' found in sys.modules after import of package 'swebench.harness', but prior to execution of 'swebench.harness.run_evaluation'; this may result in unpredictable behaviour\n",
      "Using gold predictions - ignoring predictions_path\n",
      "README.md: 100%|███████████████████████████| 3.67k/3.67k [00:00<00:00, 29.1MB/s]\n",
      "dev-00000-of-00001.parquet: 100%|████████████| 120k/120k [00:00<00:00, 5.87MB/s]\n",
      "test-00000-of-00001.parquet: 100%|█████████| 1.12M/1.12M [00:00<00:00, 34.7MB/s]\n",
      "Generating dev split: 100%|████████████| 23/23 [00:00<00:00, 3280.70 examples/s]\n",
      "Generating test split: 100%|████████| 300/300 [00:00<00:00, 25703.54 examples/s]\n",
      "Running 1 unevaluated instances...\n",
      "Building base image (sweb.base.x86_64:latest)\n",
      "Base images built successfully.\n",
      "Total environment images to build: 1\n",
      "Building environment images: 100%|████████████████| 1/1 [01:11<00:00, 71.01s/it]\n",
      "All environment images built successfully.\n",
      "Running 1 instances...\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:47<00:00, 47.21s/it]\n",
      "All instances run.\n",
      "Cleaning cached images...\n",
      "Removed 0 images.\n",
      "Total instances: 1\n",
      "Instances submitted: 1\n",
      "Instances completed: 1\n",
      "Instances incomplete: 0\n",
      "Instances resolved: 1\n",
      "Instances unresolved: 0\n",
      "Instances with empty patches: 0\n",
      "Instances with errors: 0\n",
      "Unstopped containers: 0\n",
      "Unremoved images: 0\n",
      "Report written to gold.validate-gold.json\n"
     ]
    }
   ],
   "source": [
    "#Test code for docker setup\n",
    "\n",
    "# !python -m swebench.harness.run_evaluation \\\n",
    "#     --predictions_path gold \\\n",
    "#     --max_workers 1 \\\n",
    "#     --instance_ids sympy__sympy-20590 \\\n",
    "#     --run_id validate-gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2473 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Example model, replace with your preferred one\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def predict(inputs: dict):\n",
    "    # Extract the repo and patch from the input dictionary\n",
    "    repo = inputs.get('repo', '')\n",
    "    patch = inputs.get('patch', '')\n",
    "\n",
    "    # Prepare the input text by combining repo and patch details\n",
    "    input_text = f\"Repo: {repo}\\nPatch:\\n{patch}\"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs_encoded = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Check if the input length exceeds the model's maximum length\n",
    "    max_length = 2048\n",
    "    if inputs_encoded.size(1) > max_length:\n",
    "        # Truncate the input to fit within the max length\n",
    "        inputs_encoded = inputs_encoded[:, :max_length]\n",
    "\n",
    "    # Generate predictions (patch) using the model\n",
    "    outputs = model.generate(inputs_encoded, max_length=2050, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated patch text\n",
    "    patch_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return the instance_id and generated patch\n",
    "    return {\n",
    "        \"instance_id\": inputs['instance_id'],\n",
    "        \"model_patch\": patch_text,  # This is the generated patch from the model\n",
    "        \"model_name_or_path\": model_name\n",
    "    }\n",
    "# Define the number of predictions you want to generate\n",
    "num_predictions = 3  # You can set this to the desired number of examples\n",
    "\n",
    "# Initialize a list to store all the predictions\n",
    "predictions = []\n",
    "\n",
    "# Loop through the examples in the dataset and generate predictions\n",
    "for i, example in enumerate(dataset):\n",
    "    if i >= num_predictions:\n",
    "        break  # Stop once the desired number of predictions is reached\n",
    "\n",
    "    # Generate a prediction using the predict function\n",
    "    result = predict(example)\n",
    "    \n",
    "    # Append the result to the predictions list\n",
    "    predictions.append(result)\n",
    "\n",
    "# Save the predictions to a JSON file\n",
    "with open('predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-14 11:11:02,551 - datasets - INFO - PyTorch version 2.4.1 available.\n",
      "<frozen runpy>:128: RuntimeWarning: 'swebench.harness.run_evaluation' found in sys.modules after import of package 'swebench.harness', but prior to execution of 'swebench.harness.run_evaluation'; this may result in unpredictable behaviour\n",
      "README.md: 100%|███████████████████████████| 3.88k/3.88k [00:00<00:00, 21.4MB/s]\n",
      "dev-00000-of-00001.parquet: 100%|██████████| 1.38M/1.38M [00:00<00:00, 32.0MB/s]\n",
      "test-00000-of-00001.parquet: 100%|█████████| 12.1M/12.1M [00:00<00:00, 52.3MB/s]\n",
      "train-00000-of-00001.parquet: 100%|███████████| 107M/107M [00:00<00:00, 257MB/s]\n",
      "Generating dev split: 100%|██████████| 225/225 [00:00<00:00, 9818.13 examples/s]\n",
      "Generating test split: 100%|██████| 2294/2294 [00:00<00:00, 16719.29 examples/s]\n",
      "Generating train split: 100%|███| 19008/19008 [00:01<00:00, 14070.58 examples/s]\n",
      "Running 3 unevaluated instances...\n",
      "Base image sweb.base.x86_64:latest already exists, skipping build.\n",
      "Base images built successfully.\n",
      "Total environment images to build: 1\n",
      "Building environment images: 100%|████████████████| 1/1 [01:27<00:00, 87.56s/it]\n",
      "All environment images built successfully.\n",
      "Running 3 instances...\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]Evaluation error for astropy__astropy-11693: >>>>> Patch Apply Failed:\n",
      "patching file astropy/wcs/wcsapi/fitswcs.py\n",
      "patch: **** malformed patch at line 34: +        except NoConvergence as e:\n",
      "\n",
      "\n",
      "Check (logs/run_evaluation/test/TinyLlama__TinyLlama-1.1B-Chat-v1.0/astropy__astropy-11693/run_instance.log) for more information.\n",
      " 67%|█████████████████████████████▎              | 2/3 [04:53<02:26, 146.62s/it]Evaluation error for astropy__astropy-12318: >>>>> Patch Apply Failed:\n",
      "patching file astropy/modeling/physical_models.py\n",
      "patch unexpectedly ends in middle of line\n",
      "patch: **** unexpected end of file in patch\n",
      "\n",
      "Check (logs/run_evaluation/test/TinyLlama__TinyLlama-1.1B-Chat-v1.0/astropy__astropy-12318/run_instance.log) for more information.\n",
      "100%|████████████████████████████████████████████| 3/3 [06:39<00:00, 133.22s/it]\n",
      "All instances run.\n",
      "Cleaning cached images...\n",
      "Removed 0 images.\n",
      "Total instances: 2294\n",
      "Instances submitted: 3\n",
      "Instances completed: 1\n",
      "Instances incomplete: 2291\n",
      "Instances resolved: 1\n",
      "Instances unresolved: 0\n",
      "Instances with empty patches: 0\n",
      "Instances with errors: 2\n",
      "Unstopped containers: 0\n",
      "Unremoved images: 0\n",
      "Report written to TinyLlama__TinyLlama-1.1B-Chat-v1.0.test.json\n"
     ]
    }
   ],
   "source": [
    "!python -m swebench.harness.run_evaluation \\\n",
    "    --dataset_name princeton-nlp/SWE-bench \\\n",
    "    --predictions_path predictions.json \\\n",
    "    --max_workers 1 \\\n",
    "    --run_id \"test\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
